{
    "nbformat_minor": 1, 
    "cells": [
        {
            "outputs": [
                {
                    "output_type": "execute_result", 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'\\n    Tensors\\n    Layers\\n    Neural Nets\\n    Loss Functions\\n    Optimizers\\n    Data\\n    Training\\n    XOR Example\\n    FizzBuzz Example (time permitting)\\n'"
                    }, 
                    "execution_count": 1
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 1, 
            "source": "\"\"\"\n    Tensors\n    Layers\n    Neural Nets\n    Loss Functions\n    Optimizers\n    Data\n    Training\n    XOR Example\n    FizzBuzz Example (time permitting)\n\"\"\"\n\n# https://github.com/joelgrus/odscnet\n# Since this code was writing as Python library, I need to learn how to organize it in the Notebook\n# https://stackoverflow.com/questions/36427747/scientific-computing-ipython-notebook-how-to-organize-code\n# Also look into tips to organize a Data Science project and more importantly with Jupyter Notebooks\n### https://medium.com/outlier-bio-blog/a-quick-guide-to-organizing-data-science-projects-updated-for-2016-4cbb1e6dac71\n### https://drivendata.github.io/cookiecutter-data-science/\n### https://swcarpentry.github.io/2014-01-31-ucsb/lessons/jk-python/reproducible_workflow.html\n### http://www.carlboettiger.info/2012/05/06/research-workflow.html\n\n# Note: When running in IPython Notebook, we can remove the library import"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 2, 
            "source": "\"\"\"\nA tensor is just a multidimensinoal array\n\"\"\"\nfrom numpy import ndarray as Tensor"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 3, 
            "source": "\"\"\"\nour neural nets will be made up of layers\none might look like\ninputs -> linear -> tanh -> linear -> outputs\n\"\"\"\nfrom typing import Dict, Callable\nimport numpy as np\n\n# from odscnet.tensor import Tensor\n\nclass Layer:\n    def __init__(self) -> None:\n#        self.params: Dict[str, Tensor] = {}\n#        self.grads: Dict[str, Tensor] = {}\n#        self.params[Dict[str, Tensor]] = {}\n#        self.grads[Dict[str, Tensor]] = {}\n        self.params = {}\n        self.grads = {} \n    \n    def forward(self, inputs: Tensor) -> Tensor:\n        raise NotImplementedError\n\n    def backward(self, grad: Tensor) -> Tensor:\n        raise NotImplementedError\n\nclass Linear(Layer):\n    \"\"\"\n    computes inputs @ w + b\n    \"\"\"\n    def __init__(self, input_size: int, output_size: int) -> None:\n        super().__init__()\n        self.params[\"w\"] = np.random.randn(input_size, output_size)\n        self.params[\"b\"] = np.random.randn(output_size)\n\n    def forward(self, inputs: Tensor) -> Tensor:\n        # batch_size, input_size = inputs.shape\n        self.inputs = inputs\n        return inputs @ self.params[\"w\"] + self.params[\"b\"]\n\n    def backward(self, grad: Tensor) -> Tensor:\n        \"\"\"\n        if y = f(x) and x = a * b\n        then dy/da = f'(x) * b\n        and dy/db = f'(x) * a\n        now if x = a @ b\n        then dy/da = f'(x) @ b.T\n        and dy/db = a.T @ f'(x)\n        \"\"\"\n        self.grads[\"b\"] = np.sum(grad, axis=0)\n        self.grads[\"w\"] = self.inputs.T @ grad\n        return grad @ self.params[\"w\"].T\n\nF = Callable[[Tensor], Tensor]\n\nclass Activation(Layer):\n    def __init__(self, f: F, f_prime: F) -> None:\n        super().__init__()\n        self.f = f\n        self.f_prime = f_prime\n\n    def forward(self, inputs: Tensor) -> Tensor:\n        self.inputs = inputs\n        return self.f(inputs)\n\n    def backward(self, grad: Tensor) -> Tensor:\n        return self.f_prime(self.inputs) * grad\n\ndef tanh(x: Tensor) -> Tensor:\n    return np.tanh(x)\n\ndef tanh_prime(x: Tensor) -> Tensor:\n    y = tanh(x)\n    return 1 - y ** 2\n\nclass Tanh(Activation):\n    def __init__(self):\n        super().__init__(tanh, tanh_prime)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 4, 
            "source": "\"\"\"\nA neural net is just a collection of layers\n\"\"\"\nfrom typing import Sequence, Iterator, Tuple\n\n#from odscnet.tensor import Tensor\n#from odscnet.layers import Layer\n\nclass NeuralNet:\n    def __init__(self, layers: Sequence[Layer]) -> None:\n        self.layers = layers\n\n    def forward(self, inputs: Tensor) -> Tensor:\n        for layer in self.layers:\n            inputs = layer.forward(inputs)\n        return inputs\n\n    def backward(self, grad: Tensor) -> Tensor:\n        for layer in reversed(self.layers):\n            grad = layer.backward(grad)\n        return grad\n\n    def params_and_grads(self) -> Iterator[Tuple[Tensor, Tensor]]:\n        for layer in self.layers:\n            for name, param in layer.params.items():\n                grad = layer.grads[name]\n                yield param, grad"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 6, 
            "source": "\"\"\"\nAn optimizer uses the gradients to adjust the weights\nof the neural net\n\"\"\"\n#from odscnet.nn import NeuralNet\n\nclass Optimizer:\n    def step(self, net: NeuralNet) -> None:\n        raise NotImplementedError\n\nclass SGD(Optimizer):\n    def __init__(self, lr: float = 0.01) -> None:\n        self.lr = lr\n\n    def step(self, net: NeuralNet) -> None:\n        for param, grad in net.params_and_grads():\n            param -= self.lr * grad"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 7, 
            "source": "\"\"\"\nA loss function measures how good or bad\nour predictions are, and gives us a gradient\n\"\"\"\nimport numpy as np\n\n#from odscnet.tensor import Tensor\n\nclass Loss:\n    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n        raise NotImplementedError\n\n    def grad(self, predicted: Tensor, actual: Tensor) -> Tensor:\n        raise NotImplementedError\n\n\nclass MSE(Loss):\n    \"\"\"\n    Actually total squared error\n    \"\"\"\n    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n        return np.sum((predicted - actual) ** 2)\n\n    def grad(self, predicted: Tensor, actual: Tensor) -> Tensor:\n        return 2 * (predicted - actual)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 8, 
            "source": "\"\"\"\nWe want to process our data in batches\n\"\"\"\nfrom typing import NamedTuple, Iterator\n\nimport numpy as np\n\n#from odscnet.tensor import Tensor\n\nBatch = NamedTuple(\"Batch\", [(\"inputs\", Tensor), (\"targets\", Tensor)])\n\nclass DataIterator:\n    def __call__(self, inputs: Tensor, targets: Tensor) -> Iterator[Batch]:\n        raise NotImplementedError\n\nclass BatchIterator(DataIterator):\n    def __init__(self, batch_size: int = 32, shuffle: bool = True) -> None:\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n    def __call__(self, inputs: Tensor, targets: Tensor) -> Iterator[Batch]:\n        starts = np.arange(0, len(inputs), self.batch_size)\n        if self.shuffle:\n            np.random.shuffle(starts)\n        for start in starts:\n            end = start + self.batch_size\n            batch_inputs = inputs[start:end]\n            batch_targets = targets[start:end]\n            yield Batch(batch_inputs, batch_targets)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 16, 
            "source": "\"\"\"\nHere's a function to train a neural net\n\"\"\"\n\n#from odscnet.tensor import Tensor\n#from odscnet.nn import NeuralNet\n#from odscnet.loss import Loss, MSE\n#from odscnet.optim import Optimizer, SGD\n#from odscnet.data import DataIterator, BatchIterator\n\ndef train(net: NeuralNet,\n          inputs: Tensor,\n          targets: Tensor,\n          num_epochs: int = 5000,\n          iterator: DataIterator = BatchIterator(),\n          loss: Loss = MSE(),\n          optimizer: Optimizer = SGD()) -> None:\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        for batch in iterator(inputs, targets):\n            predictions = net.forward(batch.inputs)\n            epoch_loss += loss.loss(predictions, batch.targets)\n            grad = loss.grad(predictions, batch.targets)\n            net.backward(grad)\n            optimizer.step(net)\n        if (epoch % 100) == 0:\n            print(epoch, epoch_loss)"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 10, 
            "source": "# Binary encode an Integer into array of binary values\nfrom typing import List\ndef binary_encode(x: int) -> List[int]:\n    \"\"\"\n    return x as a 10-digit binary number\n    \"\"\"\n    return [x >> i & 1 for i in range(10)]\n\n#print(binary_encode(100))\n#print(binary_encode(101))"
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 12, 
            "source": "from typing import List\ndef fizz_buzz_encode(x: int) -> List[int]:\n    if x % 15 == 0:\n        return [0, 0, 0, 1]\n    elif x % 5 == 0:\n        return [0, 0, 1, 0]\n    elif x % 3 == 0:\n        return [0, 1, 0, 0]\n    else:\n        return [1, 0, 0, 0]\n\n#print(fizz_buzz_encode(1)) \n#print(fizz_buzz_encode(3)) \n#print(fizz_buzz_encode(4)) \n#print(fizz_buzz_encode(5)) \n#print(fizz_buzz_encode(10))\n#print(fizz_buzz_encode(15))\n#print(fizz_buzz_encode(20)) "
        }, 
        {
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0 128974.471922\n100 573.567067796\n200 560.738491206\n300 537.305282075\n400 507.174444508\n500 453.53425435\n600 378.124695649\n700 348.112591679\n800 308.164533847\n900 324.820731585\n1000 238.531213196\n1100 223.601815312\n1200 190.115919604\n1300 191.493855096\n1400 153.350230791\n1500 199.230537494\n1600 129.354616411\n1700 132.459758087\n1800 112.040805283\n1900 110.277342558\n2000 97.09589892\n2100 93.5802551167\n2200 89.2317561699\n2300 82.8715292139\n2400 82.577699859\n2500 79.2305005409\n2600 80.2699885935\n2700 79.3173038208\n2800 71.8011974765\n2900 72.3444665917\n3000 67.6419629833\n3100 63.6265299629\n3200 62.7282108452\n3300 61.6941135616\n3400 61.4470951189\n3500 62.4868725369\n3600 58.1548915322\n3700 55.391529993\n3800 55.8295976218\n3900 54.3915726571\n4000 54.8773937739\n4100 54.775626096\n4200 53.4802488673\n4300 47.8599051617\n4400 45.5255881229\n4500 39.3692707952\n4600 43.5312487375\n4700 39.9271274625\n4800 36.8794582821\n4900 39.8319641214\n1 1 1   --match\n2 2 2   --match\n3 fizz fizz   --match\n4 4 4   --match\n5 buzz buzz   --match\n6 fizz fizz   --match\n7 7 7   --match\n8 8 8   --match\n9 fizz fizz   --match\n10 buzz buzz   --match\n11 11 11   --match\n12 fizz fizz   --match\n13 13 13   --match\n14 14 14   --match\n15 fizzbuzz fizzbuzz   --match\n16 16 16   --match\n17 17 17   --match\n18 fizz fizz   --match\n19 19 19   --match\n20 buzz buzz   --match\n21 fizz fizz   --match\n22 22 22   --match\n23 23 23   --match\n24 fizz fizz   --match\n25 buzz buzz   --match\n26 26 26   --match\n27 fizz fizz   --match\n28 28 28   --match\n29 29 29   --match\n30 fizzbuzz fizzbuzz   --match\n31 31 31   --match\n32 32 32   --match\n33 fizz fizz   --match\n34 34 34   --match\n35 buzz buzz   --match\n36 fizz fizz   --match\n37 37 37   --match\n38 38 38   --match\n39 fizz fizz   --match\n40 buzz buzz   --match\n41 41 41   --match\n42 fizz fizz   --match\n43 43 43   --match\n44 44 44   --match\n45 fizzbuzz fizzbuzz   --match\n46 46 46   --match\n47 47 47   --match\n48 fizz fizz   --match\n49 49 49   --match\n50 buzz buzz   --match\n51 fizz fizz   --match\n52 52 52   --match\n53 53 53   --match\n54 fizz fizz   --match\n55 buzz buzz   --match\n56 56 56   --match\n57 fizz fizz   --match\n58 58 58   --match\n59 59 59   --match\n60 fizzbuzz fizzbuzz   --match\n61 61 61   --match\n62 62 62   --match\n63 fizz fizz   --match\n64 64 64   --match\n65 buzz buzz   --match\n66 fizz fizz   --match\n67 67 67   --match\n68 68 68   --match\n69 fizz fizz   --match\n70 buzz buzz   --match\n71 71 71   --match\n72 fizz fizz   --match\n73 73 73   --match\n74 74 74   --match\n75 fizzbuzz fizzbuzz   --match\n76 76 76   --match\n77 77 77   --match\n78 fizz fizz   --match\n79 79 79   --match\n80 buzz buzz   --match\n81 fizz fizz   --match\n82 82 82   --match\n83 83 83   --match\n84 84 fizz   --miss\n85 buzz buzz   --match\n86 86 86   --match\n87 fizz fizz   --match\n88 88 88   --match\n89 89 89   --match\n90 fizzbuzz fizzbuzz   --match\n91 91 91   --match\n92 92 92   --match\n93 93 fizz   --miss\n94 94 94   --match\n95 buzz buzz   --match\n96 fizz fizz   --match\n97 97 97   --match\n98 98 98   --match\n99 fizz fizz   --match\n100 buzz buzz   --match\nScore= 98 over 100\n"
                }
            ], 
            "cell_type": "code", 
            "metadata": {}, 
            "execution_count": 17, 
            "source": "\"\"\"\nFIZZBUZZ (5000)\n\nfizz buzz is the following problem\n\ngiven an input x,\nif x % 3 == 0, print \"fizz\"\nif x % 5 == 0, print \"buzz\"\nif x % 15 == 0, print \"fizzbuzz\"\notherwise just print x\n\nTo train the model, we use binary encoding (see cells above) - why???\n  ...Train data are values from 101 to 1024\n  ...Test data are values from 1 to 100\n\n\"\"\"\nfrom typing import List\n\nimport numpy as np\n\n#from odscnet.train import train\n#from odscnet.nn import NeuralNet\n#from odscnet.layers import Linear, Tanh\n#from odscnet.optim import SGD\n\ninputs = np.array([\n    binary_encode(x) for x in range(101, 1024)\n])\n\ntargets = np.array([\n    fizz_buzz_encode(x) for x in range(101, 1024)\n])\n\nnet = NeuralNet([\n    Linear(input_size=10, output_size=50),\n    Tanh(),\n    Linear(input_size=50, output_size=4)\n])\n\ntrain(net, inputs, targets, num_epochs=5000,\n      optimizer=SGD(lr=0.001))\n\n# This is where we check the test data prediction.\n# We print strings \"fizz, buzz and fizzbuzz\" to easily recognize the match\n# With 5000 iterations, the prediction is perfect\nmatch = 0\nfor x in range(1, 101):\n    inputs = binary_encode(x)\n    prediction = net.forward(inputs)\n    \n    actual = fizz_buzz_encode(x)\n    labels = [str(x), \"fizz\", \"buzz\", \"fizzbuzz\"]\n    prediction_idx = np.argmax(prediction)\n    actual_idx = np.argmax(actual)\n    if (prediction_idx == actual_idx):\n        print(x, labels[prediction_idx], labels[actual_idx], \"  --match\")\n        match=match+1\n    else:\n        print(x, labels[prediction_idx], labels[actual_idx], \"  --miss\")\nprint(\"Score= \" + str(match) + \" over 100\") "
        }, 
        {
            "outputs": [], 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": null, 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "name": "python", 
            "file_extension": ".py", 
            "version": "3.5.2", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "pygments_lexer": "ipython3"
        }
    }, 
    "nbformat": 4
}